{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bits and bytes\n",
    "Converting text into a byte array:"
   ],
   "id": "55f47e18fd1276a8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T01:09:15.738376Z",
     "start_time": "2025-12-24T01:09:15.728608Z"
    }
   },
   "source": [
    "text = \"This is some text yes\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text yes')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we call `list()` on a `bytearray` each byte is treated as an individual object, and we get a list of integers corresponding to the byte values:",
   "id": "bcd903dfab8f2a0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T01:18:17.744802Z",
     "start_time": "2025-12-24T01:18:17.738582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)\n",
    "print(\"the number of tokens: \",len(ids))"
   ],
   "id": "f013e1210258e6a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116, 32, 121, 101, 115]\n",
      "the number of tokens:  21\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a way to turn text into a token ID representation, however creating one id for each character results in too many id's.\n",
    "\n",
    "Instead of each character BPE tokenizers have a vocabulary with a token ID for each word/subword.\n",
    "For example the GPT-2 tokenizer tokenizes \"This is some text yes\" into 5 tokens and not 21."
   ],
   "id": "c1f0a144aedcd75c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T01:19:55.757434Z",
     "start_time": "2025-12-24T01:19:48.805732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2ids = list(gpt2_tokenizer.encode(\"This is some text yes\"))\n",
    "print(gpt2ids)\n",
    "print(\"the number of tokens: \",len(gpt2ids))"
   ],
   "id": "d12df40159b6121c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420, 3763]\n",
      "the number of tokens:  5\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since there is only $2^8 = 256$ characters one byte can represent, `bytearray(range(0,257))` results in `VauleError: byte must be in range(0, 256)`\n",
    "A BPE tokenizer usually uses these 256 values as its first 256 single character tokens, we can check this if we run the code:"
   ],
   "id": "eb59cbbb32ed620b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T14:52:41.414229Z",
     "start_time": "2025-12-28T14:52:41.404330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    if 10 < i < 250 or 265 < i < 290:\n",
    "        continue #we don't want to really print all the 300 numbers since it would be unreadable\n",
    "    print(f\"{i}: {decoded}\")"
   ],
   "id": "872eab2e76da658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, some of the decoded tokens starting from 256 and so on which start with a whitespace are considered different (for example 't' is different from ' t') which has been improved in the GPT-4 tokenizer\n",
    "\n",
    "# Building the vocabulary\n",
    "\n",
    "The purpose of the BPE tokenization algorithm is to build a vocabulary of commonly occurring subwords like `298: ent` (from the words *entity, entertain, entrance, ...*) or words like\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "3763: yes\n",
    "```\n",
    "\n",
    "The general structure of the BPE algorithms goes like this:\n",
    "\n",
    "## BPE algorithm outline\n",
    "\n",
    "### 1. Identify frequent pairs\n",
    "- Every iteration scan the text for the most commonly occurring pair of bytes(characters)\n",
    "### 2. Replace and record\n",
    "- Replaces that pair with a new placeholder ID (which is not already in use, so if we start with 0,...,255, the first placeholder should be 256)\n",
    "- Records this mapping in a lookup table\n",
    "- The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (50,257 for gpt-2)\n",
    "### 3. Repeat until no gains\n",
    "- Keep repeating steps 1 and 2, merging most common pairs, until there is no pair that occurs more than once\n",
    "### Decompression (decoding)\n",
    "- to restore the original text, reverse the process by substituting each ID with the corresponding pair from the lookup table\n",
    "\n",
    "\n",
    "# BPE algorithm example\n",
    "## Concrete example of the 1st and 2nd step (encoding)\n",
    "- Let's say we want to build a vocabulary out of the sentence `the cat in the hat` which will be out training dataset\n",
    "#### Iteration 1\n",
    "1. Identifying the frequent pairs\n",
    "- In the text, `th` appears 2 times\n",
    "2. Replace and record\n",
    "- Replace the `th` with the first token not in use, e.g., `256`\n",
    "- The new text is `<256>e cat in <256>e hat`\n",
    "- the new vocabulary is:\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "#### Iteration 2\n",
    "1. Identifying the frequent pairs\n",
    "- In the text, `<256>e` appears 2 times\n",
    "2. Replace and record\n",
    "- Replace the `<256>e` with the first token not in use, e.g., `257`\n",
    "- The new text is `<257> cat in <257> hat`\n",
    "- the new vocabulary is:\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "```\n",
    "#### Iteration 3\n",
    "1. Identifying the frequent pairs\n",
    "- In the text, `<257> ` appears 2 times\n",
    "2. Replace and record\n",
    "- Replace the `<257> ` with the first token not in use, e.g., `258`\n",
    "- The new text is `<258>cat in <258>hat`\n",
    "- the new vocabulary is:\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "  258: \"<257> \"\n",
    "```\n",
    "- and so on...\n",
    "\n",
    "\n",
    "## Concrete example of the last step (decoding)\n",
    "\n",
    "To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    "- Start with the final compressed text: <258>cat in <258>hat\n",
    "- Substitute <258> → <257> : <257> cat in <257> hat\n",
    "- Substitute <257> → <256>e: <256>e cat in <256>e hat\n",
    "- Substitute <256> → “th”: the cat in the hat\n",
    "\n"
   ],
   "id": "856cbf8e4c3b7f5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "726cfcf143a3f864"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
