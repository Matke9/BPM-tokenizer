{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This BPE from scratch implementation is a result of following Sebastian Raschka's tutorial\n",
    "The link to his notebook is https://sebastianraschka.com/blog/2025/bpe-from-scratch.html\n",
    "# Bits and bytes\n",
    "Converting text into a byte array:"
   ],
   "id": "55f47e18fd1276a8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T01:09:15.738376Z",
     "start_time": "2025-12-24T01:09:15.728608Z"
    }
   },
   "source": [
    "from enum import nonmember\n",
    "from sys import prefix\n",
    "\n",
    "text = \"This is some text yes\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text yes')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we call `list()` on a `bytearray` each byte is treated as an individual object, and we get a list of integers corresponding to the byte values:",
   "id": "bcd903dfab8f2a0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T01:18:17.744802Z",
     "start_time": "2025-12-24T01:18:17.738582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)\n",
    "print(\"the number of tokens: \",len(ids))"
   ],
   "id": "f013e1210258e6a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116, 32, 121, 101, 115]\n",
      "the number of tokens:  21\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a way to turn text into a token ID representation, however creating one id for each character results in too many id's.\n",
    "\n",
    "Instead of each character BPE tokenizers have a vocabulary with a token ID for each word/subword.\n",
    "For example the GPT-2 tokenizer tokenizes \"This is some text yes\" into 5 tokens and not 21."
   ],
   "id": "c1f0a144aedcd75c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T01:19:55.757434Z",
     "start_time": "2025-12-24T01:19:48.805732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2ids = list(gpt2_tokenizer.encode(\"This is some text yes\"))\n",
    "print(gpt2ids)\n",
    "print(\"the number of tokens: \",len(gpt2ids))"
   ],
   "id": "d12df40159b6121c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420, 3763]\n",
      "the number of tokens:  5\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since there is only $2^8 = 256$ characters one byte can represent, `bytearray(range(0,257))` results in `VauleError: byte must be in range(0, 256)`\n",
    "A BPE tokenizer usually uses these 256 values as its first 256 single character tokens, we can check this if we run the code:"
   ],
   "id": "eb59cbbb32ed620b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T14:52:41.414229Z",
     "start_time": "2025-12-28T14:52:41.404330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    if 10 < i < 250 or 265 < i < 290:\n",
    "        continue #we don't want to really print all the 300 numbers since it would be unreadable\n",
    "    print(f\"{i}: {decoded}\")"
   ],
   "id": "872eab2e76da658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, some of the decoded tokens starting from 256 and so on which start with a whitespace are considered different (for example 't' is different from ' t') which has been improved in the GPT-4 tokenizer\n",
    "\n",
    "# Building the vocabulary\n",
    "\n",
    "The purpose of the BPE tokenization algorithm is to build a vocabulary of commonly occurring subwords like `298: ent` (from the words *entity, entertain, entrance, ...*) or words like\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "3763: yes\n",
    "```\n",
    "\n",
    "The general structure of the BPE algorithms goes like this:\n",
    "\n",
    "## BPE algorithm outline\n",
    "\n",
    "### 1. Identify frequent pairs\n",
    "- Every iteration scan the text for the most commonly occurring pair of bytes(characters)\n",
    "### 2. Replace and record\n",
    "- Replaces that pair with a new placeholder ID (which is not already in use, so if we start with 0,...,255, the first placeholder should be 256)\n",
    "- Records this mapping in a lookup table\n",
    "- The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (50,257 for gpt-2)\n",
    "### 3. Repeat until no gains\n",
    "- Keep repeating steps 1 and 2, merging most common pairs, until there is no pair that occurs more than once\n",
    "### Decompression (decoding)\n",
    "- to restore the original text, reverse the process by substituting each ID with the corresponding pair from the lookup table\n",
    "\n",
    "\n",
    "# BPE algorithm example\n",
    "## Concrete example of the 1st and 2nd step (encoding)\n",
    "- Let's say we want to build a vocabulary out of the sentence `the cat in the hat` which will be out training dataset\n",
    "#### Iteration 1\n",
    "1. Identifying the frequent pairs\n",
    "- In the text, `th` appears 2 times\n",
    "2. Replace and record\n",
    "- Replace the `th` with the first token not in use, e.g., `256`\n",
    "- The new text is `<256>e cat in <256>e hat`\n",
    "- the new vocabulary is:\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "#### Iteration 2\n",
    "1. Identifying the frequent pairs\n",
    "- In the text, `<256>e` appears 2 times\n",
    "2. Replace and record\n",
    "- Replace the `<256>e` with the first token not in use, e.g., `257`\n",
    "- The new text is `<257> cat in <257> hat`\n",
    "- the new vocabulary is:\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "```\n",
    "#### Iteration 3\n",
    "1. Identifying the frequent pairs\n",
    "- In the text, `<257> ` appears 2 times\n",
    "2. Replace and record\n",
    "- Replace the `<257> ` with the first token not in use, e.g., `258`\n",
    "- The new text is `<258>cat in <258>hat`\n",
    "- the new vocabulary is:\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "  258: \"<257> \"\n",
    "```\n",
    "- and so on...\n",
    "\n",
    "\n",
    "## Concrete example of the last step (decoding)\n",
    "\n",
    "To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    "- Start with the final compressed text: <258>cat in <258>hat\n",
    "- Substitute <258> → <257> : <257> cat in <257> hat\n",
    "- Substitute <257> → <256>e: <256>e cat in <256>e hat\n",
    "- Substitute <256> → “th”: the cat in the hat\n",
    "\n",
    "\n",
    "\n",
    "# A simple BPE implementation\n",
    "\n",
    "- Below is an implementation of the algorithm we described as a Python class that resembles the `tiktoken` Python interface\n",
    "- For the encoding we will use the `train()` method which is similar to the `encode()` method although not as complicated\n",
    "\n",
    "We will:\n",
    "1. Split the text into individual bytes\n",
    "2. Repeatedly find and replace adjacent tokens when they match any pair in the learned BPE merges (from highest to lowest \"rank\", the order they were learned)\n",
    "3. merge until no more merges can be applied\n",
    "4. The final list of token IDs os the encoded output"
   ],
   "id": "856cbf8e4c3b7f5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # maps token_id to token_str\n",
    "        self.vocab = {}\n",
    "        # maps token_str to token_id\n",
    "        self.inverse_vocab = {}\n",
    "        # dictionary of BPE merges\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "        # we are using a rank dict like the official OpenAI GPT-2\n",
    "        # {(string_A, string_B): rank}, lower rank = higher priority\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (dict): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with \"Ġ\"\n",
    "        # The \"Ġ\" is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # GPT-4 BPE would tokenize it as [\"Hello\", \" world\"]\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i !=0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text)) if char not in allowed_special\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.reversed_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json')\n",
    "            bpe_merges_path (str): Path to the bpe_merges file (GPT-2 calls it 'vocab.bpe')\n",
    "        \"\"\"\n",
    "\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # Convert loaded vocabulary to correct format\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Handle newline character without adding a new token\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # Use an existing token ID as a placeholder for '\\n'\n",
    "            # Preferentially use \"<|endoftext|>\" if available\n",
    "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                # If no fallback token is available, raise an error\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "\n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id] = \"\\n\"\n",
    "\n",
    "        # Load GPT-2 merges and store them with an assigned \"rank\"\n",
    "        self.bpe_ranks = {}\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) == 2:\n",
    "                    token1, token2 = pair\n",
    "                    # If token1 or token2 not in vocab, skip\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                        self.bpe_ranks[(token1, token2)] = rank\n",
    "                        rank += 1\n",
    "                    else:\n",
    "                        print(f\"Skipping pair: {pair} as one token is not in the vocabulary.\")\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "\n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "\n",
    "        import re\n",
    "\n",
    "        token_ids = []\n",
    "\n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Build regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "\n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index : match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None)) # Encode prefix without special handling\n",
    "\n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token: {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "\n",
    "            text = text[last_index:]\n",
    "\n",
    "            # Check if any disallowed special tokens are in the remainder\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "\n",
    "        # If no special tokens, or remaining text after soecial token split:\n",
    "        tokens = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:\n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "                elif j == 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "\n",
    "        return token_ids\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Token {token} is missing token IDs: {missing_chars}\")\n",
    "\n",
    "        # If we haven't loaded OpenAI's GPT-2 merges, we use Sebastian's approach\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(token_ids) - 1:\n",
    "                    pair = (token_ids[i], token_ids[i + 1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                        i += 2  # Skip the next token as it's merged\n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                if i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids = new_tokens\n",
    "            return token_ids\n",
    "\n",
    "        # Otherwise, do GPT-2-style merging with the ranks:\n",
    "        # Convert token_ids back to string \"symbols\" for each ID\n",
    "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
    "\n",
    "        # Repeatedly merge all occurrences of the lowest-rank pair\n",
    "        while True:\n",
    "            # Collect all adjacent pairs\n",
    "            pairs = set(zip(symbols, symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # Find the pair with the best (lowest) rank\n",
    "            min_rank = float(\"inf\")\n",
    "            bigram = None\n",
    "            for p in pairs:\n",
    "                r = self.bpe_ranks.get(p, float(\"inf\"))\n",
    "                if r < min_rank:\n",
    "                    min_rank = r\n",
    "                    bigram = p\n",
    "\n",
    "            # If no valid ranked pair is present, we are done\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            # Merge all occurrences of that pair\n",
    "            first, second = bigram\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                # If we see (first, second) at position i, merge them\n",
    "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i + 1] == second:\n",
    "                    new_symbols.append(first + second) # merged symbol\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "\n",
    "            if len(symbols) == 1:\n",
    "                break\n",
    "\n",
    "        # Finally, convert merged symbols back to IDs\n",
    "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if decoded_string and not decoded_string.endswith(\" \"):\n",
    "                    decoded_string += \" \" # Add space if not present before a newline\n",
    "                decoded_string += token\n",
    "            elif token.startswith(\"Ġ\"):\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "        # Load bpe merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge[\"new_id\"]\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        match(mode):\n",
    "            case \"most\":\n",
    "                return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            case \"least\":\n",
    "                return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ],
   "id": "aadc4db842166363"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- There is too much code in the class above to discuss it in this notebook, however the next section offers a overview of the usage to better understand the class methods",
   "id": "37821e34e292ee9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
