{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Converting text into a byte array:",
   "id": "55f47e18fd1276a8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T01:09:15.738376Z",
     "start_time": "2025-12-24T01:09:15.728608Z"
    }
   },
   "source": [
    "text = \"This is some text yes\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text yes')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we call `list()` on a `bytearray` each byte is treated as an individual object, and we get a list of integers corresponding to the byte values:",
   "id": "bcd903dfab8f2a0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T01:18:17.744802Z",
     "start_time": "2025-12-24T01:18:17.738582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)\n",
    "print(\"the number of tokens: \",len(ids))"
   ],
   "id": "f013e1210258e6a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116, 32, 121, 101, 115]\n",
      "the number of tokens:  21\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a way to turn text into a token ID representation, however creating one id for each character results in too many id's.\n",
    "\n",
    "Instead of each character BPE tokenizers have a vocabulary with a token ID for each word/subword.\n",
    "For example the GPT-2 tokenizer tokenizes \"This is some text yes\" into 5 tokens and not 21."
   ],
   "id": "c1f0a144aedcd75c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T01:19:55.757434Z",
     "start_time": "2025-12-24T01:19:48.805732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2ids = list(gpt2_tokenizer.encode(\"This is some text yes\"))\n",
    "print(gpt2ids)\n",
    "print(\"the number of tokens: \",len(gpt2ids))"
   ],
   "id": "d12df40159b6121c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420, 3763]\n",
      "the number of tokens:  5\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since there is only $2^8 = 256$ characters one byte can represent, `bytearray(range(0,257))` results in `VauleError: byte must be in range(0, 256)`\n",
    "A BPE tokenizer usually uses these 256 values as its first 256 single character tokens, we can check this if we run the code:"
   ],
   "id": "eb59cbbb32ed620b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T01:52:15.608916Z",
     "start_time": "2025-12-25T01:52:15.599526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(266):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    if 10 < i < 250:\n",
    "        continue #we don't want to really print 300 numbers since it would be unreadable\n",
    "    print(f\"{i}: {decoded}\")"
   ],
   "id": "872eab2e76da658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, some of the decoded tokens starting from 256 and so on which start with a whitespace are considered different (for example 't' is different from ' t') which has been improved in the GPT-4 tokenizer",
   "id": "856cbf8e4c3b7f5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
